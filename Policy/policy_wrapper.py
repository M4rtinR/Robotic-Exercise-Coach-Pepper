from Policy.policy import Policy


class PolicyWrapper:
    """
    A class which acts as an interface between the raw policy and the behaviour tree. It can give the tree a behaviour
    or observation generated by the policy which is within a set of defined valid behaviours for a given state.
    ...
    Attributes
    ----------
    Goal Levels :type int
        6 different goal levels which correspond to stages in the interaction and the goal hierarchy of the racketware
        guide.
    Phases :type int
        2 different phases (start and end) which correspond respectively to either an intro or feedback sequence.
    Performance Levels :type int
        8 different performance levels which represent how the user did in their previous action.
    policy
        The policy we will query to obtain behaviours and observations.

    Methods
    -------
    get_behaviour(state, goal_level, performance, phase)
        Obtain a behaviour from the underlying policy and check it is valid in the current state of interaction.
    _get_valid_list(goal_level, performance, phase)
        Local method which creates the list of valid for each state of interaction.
    get_observation(state, behaviour)
        Obtain an observation from the underlying policy.
    """
    def __init__(self, belief):
        self.policy = Policy(belief)

    # Goal Levels
    PERSON_GOAL = 0
    SESSION_GOAL = 1
    EXERCISE_GOAL = 2
    STAT_GOAL = 3
    SET_GOAL = 4
    ACTION_GOAL = 5
    BASELINE_GOAL = 6

    # Phases
    PHASE_START = 0
    PHASE_END = 1

    # Performance Levels
    MET = 0             # Met the target
    MUCH_IMPROVED = 1   # Moved a lot closer to the target
    IMPROVED = 2        # Moved closer to the target
    IMPROVED_SWAP = 3   # Moved closer to the target but passed it
    STEADY = 4          # Stayed the same
    REGRESSED = 5       # Moved further away from the target
    REGRESSED_SWAP = 6  # Moved past the target and further from it
    MUCH_REGRESSED = 7  # Moved a lot further away from the target

    def get_behaviour(self, state, goal_level, performance, phase):
        """
        Obtain a behaviour from the underlying policy and check it is valid in the current state of interaction.
        :param state :type int: the previously observed state.
        :param goal_level :type int: the current goal level of the interaction.
        :param performance :type int: the performance of the user on their last action.
        :param phase :type int: the phase of the current goal level (either intro or feedback).
        :return behaviour :type int: the behaviour generated by the policy.
        """
        valid_behaviours = self._get_valid_list(goal_level, performance, phase)
        behaviour = self.policy.sample_action(state)
        while not(behaviour in valid_behaviours):
            if goal_level == self.ACTION_GOAL:     # If between shots, silence is an appropriate action so each time a
                behaviour = self.policy.A_SILENCE  # non-valid action is proposed, just use silence.
            else:
                behaviour = self.policy.sample_action(state)

        return behaviour

    def _get_valid_list(self, goal_level, performance, phase):
        """
        Local method which creates the list of valid for each state of interaction.
        :param goal_level :type int: the current goal level of the interaction.
        :param performance :type int: the performance of the user on their last action.
        :param phase :type int: the phase of the current goal level (either intro or feedback).
        :return valid_list :type list[int]: a list of valid behaviours in the current interaction state.
        """
        valid_list = []

        print('goal_level = ' + str(goal_level))
        # Person Goal
        if goal_level == self.PERSON_GOAL:
            print('Creating list for person goal')
            valid_list.extend([self.policy.A_PREINSTRUCTION, self.policy.A_PREINSTRUCTION_FIRSTNAME])

        # Baseline Goal
        elif goal_level == self.BASELINE_GOAL:
            if phase == self.PHASE_START:
                valid_list.extend([self.policy.A_PREINSTRUCTION, self.policy.A_PREINSTRUCTION_QUESTIONING,
                                   self.policy.A_PREINSTRUCTION_FIRSTNAME,
                                   self.policy.A_PREINSTRUCTION_POSITIVEMODELING,
                                   self.policy.A_POSITIVEMODELING_PREINSTRUCTION])
            else:
                valid_list.append(self.policy.A_PRAISE)

        # Session, Exercise, Stat and Set Goals will all have the same action categories (different individual actions)
        elif goal_level == self.SESSION_GOAL or goal_level == self.EXERCISE_GOAL or goal_level == self.STAT_GOAL \
                or goal_level == self.SET_GOAL:
            valid_list.extend([self.policy.A_POSTINSTRUCTIONPOSITIVE, self.policy.A_POSTINSTRUCTIONNEGATIVE,
                               self.policy.A_QUESTIONING, self.policy.A_POSTINSTRUCTIONPOSITIVE_QUESTIONING,
                               self.policy.A_POSTINSTRUCTIONPOSITIVE_FIRSTNAME,
                               self.policy.A_POSTINSTRUCTIONNEGATIVE_QUESTIONING, self.policy.A_QUESTIONING_FIRSTNAME,
                               self.policy.A_POSTINSTRUCTIONNEGATIVE_FIRSTNAME,
                               self.policy.A_QUESTIONING_POSITIVEMODELING, self.policy.A_POSITIVEMODELING_QUESTIONING,
                               self.policy.A_POSTINSTRUCTIONPOSITIVE_POSITIVE_MODELING,
                               self.policy.A_POSTINSTRUCTIONPOSITIVE_NEGATIVE_MODELING,
                               self.policy.A_POSTINSTRUCTIONNEGATIVE_POSITIVEMODELING,
                               self.policy.A_POSTINSTRUCTIONNEGATIVE_NEGATIVEMODELING,
                               self.policy.A_QUESTIONING_NEGATIVEMODELING,
                               self.policy.A_POSITIVEMODELING_POSTINSTRUCTIONPOSITIVE,
                               self.policy.A_NEGATIVEMODELING_POSTINSTRUCTIONNEGATIVE])
            if phase == self.PHASE_START:
                valid_list.extend([self.policy.A_PREINSTRUCTION, self.policy.A_PREINSTRUCTION_QUESTIONING,
                                   self.policy.A_PREINSTRUCTION_FIRSTNAME,
                                   self.policy.A_PREINSTRUCTION_POSITIVEMODELING,
                                   self.policy.A_PREINSTRUCTION_NEGATIVEMODELING,
                                   self.policy.A_POSITIVEMODELING_PREINSTRUCTION])
                if performance == self.MET:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.MUCH_IMPROVED:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.IMPROVED:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.IMPROVED_SWAP:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.STEADY:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.REGRESSED:
                    valid_list.extend([self.policy.A_SCOLD, self.policy.A_CONSOLE, self.policy.A_SCOLD_FIRSTNAME,
                                       self.policy.A_CONSOLE_FIRSTNAME])
                elif performance == self.REGRESSED_SWAP:
                    valid_list.extend([self.policy.A_SCOLD, self.policy.A_CONSOLE, self.policy.A_SCOLD_FIRSTNAME,
                                       self.policy.A_CONSOLE_FIRSTNAME])
                else:  # performance == self.MUCH_REGRESSED
                    valid_list.extend([self.policy.A_SCOLD, self.policy.A_CONSOLE, self.policy.A_SCOLD_FIRSTNAME,
                                       self.policy.A_CONSOLE_FIRSTNAME])
            else:  # phase == self.PHASE_END
                valid_list.append(self.policy.A_END)
                if performance == self.MET:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.MUCH_IMPROVED:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.IMPROVED:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.IMPROVED_SWAP:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.STEADY:
                    valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                       self.policy.A_POSITIVEMODELING_PRAISE])
                elif performance == self.REGRESSED:
                    valid_list.extend([self.policy.A_SCOLD, self.policy.A_CONSOLE, self.policy.A_SCOLD_FIRSTNAME,
                                       self.policy.A_CONSOLE_FIRSTNAME])
                elif performance == self.REGRESSED_SWAP:
                    valid_list.extend([self.policy.A_SCOLD, self.policy.A_CONSOLE, self.policy.A_SCOLD_FIRSTNAME,
                                       self.policy.A_CONSOLE_FIRSTNAME])
                else:  # performance == self.MUCH_REGRESSED
                    valid_list.extend([self.policy.A_SCOLD, self.policy.A_CONSOLE, self.policy.A_SCOLD_FIRSTNAME,
                                       self.policy.A_CONSOLE_FIRSTNAME])

        # Action Goal (each shot in squash or movement in rehab)
        else:  # goal_level == self.ACTION_GOAL:
            valid_list.extend([self.policy.A_SILENCE, self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE,
                               self.policy.A_QUESTIONING, self.policy.A_POSITIVEMODELING, self.policy.A_HUSTLE,
                               self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_QUESTIONING,
                               self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_FIRSTNAME,
                               self.policy.A_QUESTIONING_FIRSTNAME, self.policy.A_HUSTLE_FIRSTNAME,
                               self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_POSITIVEMODELING,
                               self.policy.A_POSITIVEMODELLING_HUSTLE])
            # No phases in action goals, just a behaviour after each shot.
            if performance == self.MET:
                valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_PRAISE,
                                   self.policy.A_POSITIVEMODELING_PRAISE])
            elif performance == self.MUCH_IMPROVED:
                valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_PRAISE,
                                   self.policy.A_POSITIVEMODELING_PRAISE])
            elif performance == self.IMPROVED:
                valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_PRAISE,
                                   self.policy.A_POSITIVEMODELING_PRAISE])
            elif performance == self.IMPROVED_SWAP:
                valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_PRAISE,
                                   self.policy.A_POSITIVEMODELING_PRAISE])
            elif performance == self.STEADY:
                valid_list.extend([self.policy.A_PRAISE, self.policy.A_PRAISE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONPOSITIVE_PRAISE,
                                   self.policy.A_POSITIVEMODELING_PRAISE])
            elif performance == self.REGRESSED:
                valid_list.extend([self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE, self.policy.A_NEGATIVEMODELING,
                                   self.policy.A_SCOLD, self.policy.A_CONSOLE,
                                   self.policy.A_QUESTIONING_NEGATIVEMODELING, self.policy.A_SCOLD_POSITIVEMODELING,
                                   self.policy.A_SCOLD_FIRSTNAME, self.policy.A_CONSOLE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE_NEGATIVEMODELING,
                                   self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE_FIRSTNAME,
                                   self.policy.A_POSITIVEMODELING_CONCURRENTINSTRUCTIONNEGATIVE])
            elif performance == self.REGRESSED_SWAP:
                valid_list.extend([self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE, self.policy.A_NEGATIVEMODELING,
                                   self.policy.A_SCOLD, self.policy.A_CONSOLE,
                                   self.policy.A_QUESTIONING_NEGATIVEMODELING, self.policy.A_SCOLD_POSITIVEMODELING,
                                   self.policy.A_SCOLD_FIRSTNAME, self.policy.A_CONSOLE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE_NEGATIVEMODELING,
                                   self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE_FIRSTNAME,
                                   self.policy.A_POSITIVEMODELING_CONCURRENTINSTRUCTIONNEGATIVE])
            else:  # performance == self.MUCH_REGRESSED
                valid_list.extend([self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE, self.policy.A_NEGATIVEMODELING,
                                   self.policy.A_SCOLD, self.policy.A_CONSOLE,
                                   self.policy.A_QUESTIONING_NEGATIVEMODELING, self.policy.A_SCOLD_POSITIVEMODELING,
                                   self.policy.A_SCOLD_FIRSTNAME, self.policy.A_CONSOLE_FIRSTNAME,
                                   self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE_NEGATIVEMODELING,
                                   self.policy.A_CONCURRENTINSTRUCTIONNEGATIVE_FIRSTNAME,
                                   self.policy.A_POSITIVEMODELING_CONCURRENTINSTRUCTIONNEGATIVE])

        return valid_list

    def get_observation(self, state, behaviour):
        """
        Obtain an observation from the underlying policy.
        :param state :type int: the previously observed state.
        :param behaviour :type int: the previous behaviour generated by the policy.
        :return:type int: the observation of which state we have moved to, generated by the policy.
        """
        return self.policy.sample_observation(state, behaviour)
