import logging
import os
from abc import ABC
from datetime import time

import gym
from gym import spaces
import numpy as np
from typing import Optional
# from gym.utils.renderer import Renderer
from CoachingBehaviourTree import controller, nodes, config
from Policy.policy import Policy
from task_behavior_engine.tree import NodeStatus

from Policy.policy_wrapper import PolicyWrapper


class CoachingEnvironment(gym.Env, ABC):
    """
    A class which acts as an interface between the raw policy and the behaviour tree. It can give the tree a behaviour
    or observation generated by the policy which is within a set of defined valid behaviours for a given state.
    ...
    Attributes
    ----------
    Goal Levels :type int
        6 different goal levels which correspond to stages in the interaction and the goal hierarchy of the racketware
        guide.
    Phases :type int
        2 different phases (start and end) which correspond respectively to either an intro or feedback sequence.
    Performance Levels :type int
        8 different performance levels which represent how the user did in their previous action.
    policy
        The policy we will query to obtain behaviours and observations.

    Methods
    -------
    get_behaviour(state, goal_level, performance, phase)
        Obtain a behaviour from the underlying policy and check it is valid in the current state of interaction.
    _get_valid_list(goal_level, performance, phase)
        Local method which creates the list of valid for each state of interaction.
    get_observation(state, behaviour)
        Obtain an observation from the underlying policy.
    """
    def __init__(self, render_mode: Optional[str] = None):
        self.coaching_tree = controller.create_coaching_tree()
        self.observation_space = spaces.Discrete(69)
        self.action_space = spaces.Discrete(68)  # All the possible states but without A_START
        self.policy = None

    def reset(self, seed=None, return_info=False, options=None):
        # Will be called at the start of a new session, so load the policy from file, or choose policy if first session.
        super().reset(seed=seed)

        filename = "/home/martin/PycharmProjects/coachingPolicies/AdaptedPolicies/" + config.participant_filename
        if os.path.exists(filename):
            f = open(filename, "r")
            matrix = f.readlines()
            f.close()
            observation = 0
            self.policy = PolicyWrapper(policy=matrix)
        else:
            # TODO: check this is the correct measure for choosing the initial policy.
            belief_distribution = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] if config.impairment < 4 else [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
            observation = 0
            self.policy = PolicyWrapper(belief=belief_distribution)

        return observation, self.policy

    def _get_start(self, style):
        if style == 0:
            return 0
        elif style == 1:
            return 45
        elif style == 2:
            return 90
        elif style == 3:
            return 135
        elif style == 4:
            return 180
        elif style == 5:
            return 225
        elif style == 6:
            return 270
        elif style == 7:
            return 323
        elif style == 8:
            return 376
        elif style == 9:
            return 429
        elif style == 10:
            return 482
        else:
            return 535

    def step(self, action, state):

        done = False

        config.behaviour_displayed = False
        # config.behaviour = 1

        # print("config.behaviour = " + str(config.behaviour))
        while not config.behaviour_displayed:  # Keep ticking the tree until a behaviour is given by the robot. This is the point the controller can select a new action and learn.
            result = self.coaching_tree.tick()
            if config.behaviour_displayed:
                print("Tree ticked, not returning: " + str(result))
            else:
                print("Tree ticked, returning: " + str(result))
            logging.debug(result)

        observation = self.policy.get_observation(state, action)
        reward = self._calculate_reward(action, observation, config.score, config.target)

        if action == config.A_END and config.goal_level == config.PERSON_GOAL:
            done = True

        return observation, reward, done, result

    def _calculate_reward(self, action, observation, score, target):
        # Dummy reward
        if score is None:
            return 0
        else:
            # Calculate reward based on how close score is to target
            if score == target:
                return 1
            elif score == 0:
                return -1
            elif score > target:
                return 0.5
            else:
                return -0.5

        # Option 1
        '''
        if score is None:
            if action == config.A_QUESTIONING etc:
                if response == 1:
                    reward = 1
                else:
                    reward = -1
            elif config.dcecision_overriden:
                reward = -10
        else:
            reward = 1
        '''
