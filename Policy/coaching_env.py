import logging
import os
from abc import ABC
from datetime import time

import gym
from gym import spaces
import numpy as np
from typing import Optional
# from gym.utils.renderer import Renderer
from CoachingBehaviourTree import controller, nodes, config
from Policy.policy import Policy
from task_behavior_engine.tree import NodeStatus

from Policy.policy_wrapper import PolicyWrapper


class CoachingEnvironment(gym.Env, ABC):
    """
    A class which acts as an interface between the raw policy and the behaviour tree. It can give the tree a behaviour
    or observation generated by the policy which is within a set of defined valid behaviours for a given state.
    ...
    Attributes
    ----------
    Goal Levels :type int
        6 different goal levels which correspond to stages in the interaction and the goal hierarchy of the racketware
        guide.
    Phases :type int
        2 different phases (start and end) which correspond respectively to either an intro or feedback sequence.
    Performance Levels :type int
        8 different performance levels which represent how the user did in their previous action.
    policy
        The policy we will query to obtain behaviours and observations.

    Methods
    -------
    get_behaviour(state, goal_level, performance, phase)
        Obtain a behaviour from the underlying policy and check it is valid in the current state of interaction.
    _get_valid_list(goal_level, performance, phase)
        Local method which creates the list of valid for each state of interaction.
    get_observation(state, behaviour)
        Obtain an observation from the underlying policy.
    """
    def __init__(self, render_mode: Optional[str] = None):
        self.coaching_tree = controller.create_coaching_tree()
        self.observation_space = spaces.Discrete(69)
        self.action_space = spaces.Discrete(68)  # All the possible states but without A_START
        self.policy = None

    def reset(self, seed=None, return_info=False, options=None):
        # Will be called at the start of a new session, so load the policy from file, or choose policy if first session.
        super().reset(seed=seed)

        filename = "/home/martin/PycharmProjects/coachingPolicies/AdaptedPolicies/" + config.participant_filename
        if os.path.exists(filename):
            f = open(filename, "r")
            matrix = f.readlines()
            f.close()
            observation = 0
            self.policy = PolicyWrapper(policy=matrix)
        else:
            belief_distribution = []
            if config.policy == -1:
                belief_distribution = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] if config.motivation > 5 else [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
            else:
                for i in range(12):
                    if config.policy == i:
                        belief_distribution.append(1)
                    else:
                        belief_distribution.append(0)
            observation = 0
            logging.info("Belief distribution = " + str(belief_distribution))
            self.policy = PolicyWrapper(belief=belief_distribution)
            logging.info("Policy = " + str(self.policy))

        return observation, self.policy

    def _get_start(self, style):
        if style == 0:
            return 0
        elif style == 1:
            return 45
        elif style == 2:
            return 90
        elif style == 3:
            return 135
        elif style == 4:
            return 180
        elif style == 5:
            return 225
        elif style == 6:
            return 270
        elif style == 7:
            return 323
        elif style == 8:
            return 376
        elif style == 9:
            return 429
        elif style == 10:
            return 482
        else:
            return 535

    def step(self, action, state):

        done = False

        config.behaviour_displayed = False
        # config.behaviour = 1

        # logging.debug("config.behaviour = " + str(config.behaviour))
        while not config.behaviour_displayed:  # Keep ticking the tree until a behaviour is given by the robot. This is the point the controller can select a new action and learn.
            result = self.coaching_tree.tick()
            if config.behaviour_displayed:
                logging.debug("Tree ticked, not returning: " + str(result))
            else:
                logging.debug("Tree ticked, returning: " + str(result))
            logging.debug(result)

        observation = self.policy.get_observation(state, action)
        reward = self._calculate_reward(action, observation, config.score, config.target)

        if action == config.A_END and config.goal_level == config.PERSON_GOAL:
            done = True

        return observation, reward, done, result

    def _calculate_reward(self, action, observation, score, target):
        # Dummy reward
        if score is None:
            return 0
        else:
            # Calculate reward based on how close score is to target
            if score == target:
                return 1
            elif score == 0:
                return -1
            elif score > target:
                return 0.5
            else:
                return -0.5

        # Option 1
        '''
        if score is None:
            if action == PolicyWrapper.A_QUESTIONING and controller.question_type = controller.FEEDBACK_QUESTION:
                if controller.question_feedback = 1:
                    return 0.2
                else:
                    return -0.2
            elif action == PolicyWrapper.A_QUESTIONING or action == PolicyWrapper.A_PRE-INSTRUCTION:
                if controller.decision_overriden:
                    return -1  # If policy's decision to select shots for user or have user select shots, is overriden, receive a large negative reward.
            else:
                return None  # No actions from user so reward is None and policy does not change.
        else:
            # Receive a positive reward for completing each rep, and a large positive reward for completing a set.
            if config.rep_count < config.max_reps:
                return 0.2
            else:
                return 1
        '''

        # Option 2
        '''
        if score is None:
            if action == PolicyWrapper.A_QUESTIONING and controller.question_type = controller.FEEDBACK_QUESTION:
                if controller.question_feedback = 1:
                    return 0.2
                else:
                    return -0.2
            elif action == PolicyWrapper.A_QUESTIONING or action == PolicyWrapper.A_PRE-INSTRUCTION:
                if controller.decision_overriden:
                    return -1  # If policy's decision to select shots for user or have user select shots, is overriden, receive a large negative reward.
            else:
                return None  # No actions from user so reward is None and policy does not change.
        else:
            # Reward based on improvement since last time. If more reps are done than last time, receive a reward.
            if config.rep_count < config.max_reps and config.rep_count > config.previous_best:
                return 0.5
            elif config.rep_count == config.max_reps:
                return 1  # Large reward for completing set.
        '''
